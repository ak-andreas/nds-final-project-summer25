{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d914862a",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline\n",
    "\n",
    "\n",
    "The goal of the preprocessing pipeline here is to serve as a draft. Before we perform spike inference. This could be using oases or other more sophisticated method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7f8058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from scipy.signal import savgol_filter, decimate\n",
    "from scipy.ndimage import percentile_filter\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import HuberRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88f76c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as U\n",
    "import importlib \n",
    "importlib.reload(U)\n",
    "\n",
    "data  = U.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6595e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_sparse_epochs(data):\n",
    "    \"\"\"\n",
    "    Return boolean mask over timepoints belonging to sparse-noise epochs.\n",
    "    Works if stim_epoch_table is a (n_epochs,3) object array with one string column\n",
    "    and two numeric columns (start_frame, end_frame).\n",
    "    \"\"\"\n",
    "    t = data['t']\n",
    "    mask = np.zeros_like(t, dtype=bool)\n",
    "    epochs = data['stim_epoch_table']  # shape (n_epochs, 3)\n",
    "\n",
    "    # detect which column is text\n",
    "    first = epochs[0]\n",
    "    name_col = next(i for i,x in enumerate(first) if isinstance(x, str))\n",
    "    num_cols = [i for i,x in enumerate(first) if isinstance(x, numbers.Number)]\n",
    "    if len(num_cols) != 2:\n",
    "        raise ValueError(f\"Expected 2 numeric cols, got {num_cols}\")\n",
    "    start_col, end_col = num_cols\n",
    "\n",
    "    names  = epochs[:, name_col].astype(str)\n",
    "    is_sp  = np.char.find(names, 'sparse') >= 0\n",
    "\n",
    "    for row in epochs[is_sp]:\n",
    "        start = int(row[start_col])\n",
    "        end   = int(row[end_col])\n",
    "        mask[start:end] = True\n",
    "\n",
    "    return mask\n",
    "\n",
    "def compute_qc_metrics(dff):\n",
    "    \"\"\"\n",
    "    Compute per-cell ΔF/F variance and a simple SNR metric.\n",
    "    Returns (variance array, snr array) of length n_cells.\n",
    "    \"\"\"\n",
    "    var = np.var(dff, axis=1)\n",
    "    p98 = np.percentile(dff, 98, axis=1)\n",
    "    p02 = np.percentile(dff,  2, axis=1)\n",
    "    noise_std = np.std(dff, axis=1)\n",
    "    snr = (p98 - p02) / (noise_std + 1e-8)\n",
    "    return var, snr\n",
    "def estimate_neuropil_proxy(dff):\n",
    "    \"\"\"\n",
    "    Simple proxy neuropil for each cell: mean of all other cells.\n",
    "    Inputs:\n",
    "      dff: (n_cells, n_time) ΔF/F traces\n",
    "    Returns:\n",
    "      neuropil: (n_cells, n_time)\n",
    "    \"\"\"\n",
    "    total = np.sum(dff, axis=0, keepdims=True)\n",
    "    neuropil = (total - dff) / (dff.shape[0] - 1)\n",
    "    return neuropil\n",
    "\n",
    "def regress_neuropil_robust(dff, neuropil, clamp=(0.5, 0.9)):\n",
    "    \"\"\"\n",
    "    Perform per-cell robust regression of dff_i ~ ρ_i * neuropil_i + intercept,\n",
    "    then subtract ρ_i * neuropil_i to obtain cleaned traces.\n",
    "\n",
    "    Inputs:\n",
    "      dff:      (n_cells, n_time) raw or sparse ΔF/F\n",
    "      neuropil: (n_cells, n_time) neuropil proxy traces\n",
    "      clamp:    (min, max) ρ value bounds\n",
    "\n",
    "    Returns:\n",
    "      cleaned:  (n_cells, n_time) neuropil-regressed traces\n",
    "      coefs:    (n_cells,)    array of ρ_i values\n",
    "    \"\"\"\n",
    "\n",
    "    n_cells = dff.shape[0]\n",
    "    cleaned = np.zeros_like(dff)\n",
    "    coefs   = np.zeros(n_cells)\n",
    "\n",
    "    for i in range(n_cells):\n",
    "        y = dff[i]\n",
    "        X = neuropil[i][:, None]\n",
    "        model = HuberRegressor().fit(X, y)\n",
    "        rho = model.coef_[0]\n",
    "        rho_clamped = np.clip(rho, clamp[0], clamp[1])\n",
    "        cleaned[i] = y - rho_clamped * neuropil[i]\n",
    "        coefs[i]   = rho_clamped\n",
    "\n",
    "    return cleaned, coefs\n",
    "\n",
    "def sliding_baseline(arr, t, window_sec=60, pct=10, mode=\"nearest\"):\n",
    "    \"\"\"\n",
    "    Compute and subtract a running-percentile baseline from each row of `arr`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : array_like, shape (n_cells, n_time)\n",
    "        The ΔF/F traces to baseline-correct.\n",
    "    t : array_like, shape (n_time,)\n",
    "        Time vector in seconds.\n",
    "    window_sec : float\n",
    "        Length of the sliding window in seconds.\n",
    "    pct : float\n",
    "        Percentile to use for baseline (e.g. 10).\n",
    "    mode : str\n",
    "        How to handle boundaries in scipy.ndimage.percentile_filter.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    baseline : ndarray, shape (n_cells, n_time)\n",
    "        The running baseline.\n",
    "    corrected : ndarray, shape (n_cells, n_time)\n",
    "        `arr - baseline`.\n",
    "    win_frames : int\n",
    "        Number of frames in the sliding window.\n",
    "    \"\"\"    \n",
    "\n",
    "    dt = np.median(np.diff(t))\n",
    "    win_frames = int(np.round(window_sec / dt))\n",
    "    if win_frames % 2 == 0:\n",
    "        win_frames += 1\n",
    "\n",
    "    baseline = np.zeros_like(arr)\n",
    "    for i in range(arr.shape[0]):\n",
    "        baseline[i] = percentile_filter(arr[i], percentile=pct, size=win_frames, mode=mode)\n",
    "\n",
    "    corrected = arr - baseline\n",
    "    return baseline, corrected, win_frames\n",
    "\n",
    "def smooth_dff_savgol(dff: np.ndarray, window: int, polyorder: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply a Savitzky–Golay filter to each cell’s ΔF/F trace.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dff : array, shape (n_cells, n_time)\n",
    "        The neuropil‐regressed, drift‐corrected ΔF/F traces.\n",
    "    window : int\n",
    "        Length of filter window (must be odd).\n",
    "    polyorder : int\n",
    "        Order of the polynomial fit (must be < window).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dff_smooth : array, shape (n_cells, n_time)\n",
    "        The smoothed traces.\n",
    "    \"\"\"\n",
    "    # enforce odd window\n",
    "    if window % 2 == 0:\n",
    "        raise ValueError(\"window must be odd\")\n",
    "    # apply filter along time axis=1\n",
    "    return savgol_filter(dff, window_length=window, polyorder=polyorder, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b234f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_sparse_epochs(data):\n",
    "    \"\"\"\n",
    "    Return boolean mask over timepoints belonging to sparse-noise epochs.\n",
    "    Works if stim_epoch_table is a (n_epochs,3) object array with one string column\n",
    "    and two numeric columns (start_frame, end_frame).\n",
    "    \"\"\"\n",
    "    t = data['t']\n",
    "    mask = np.zeros_like(t, dtype=bool)\n",
    "    epochs = data['stim_epoch_table']  # shape (n_epochs, 3)\n",
    "\n",
    "    # detect which column is text\n",
    "    first = epochs[0]\n",
    "    name_col = next(i for i,x in enumerate(first) if isinstance(x, str))\n",
    "    num_cols = [i for i,x in enumerate(first) if isinstance(x, numbers.Number)]\n",
    "    if len(num_cols) != 2:\n",
    "        raise ValueError(f\"Expected 2 numeric cols, got {num_cols}\")\n",
    "    start_col, end_col = num_cols\n",
    "\n",
    "    names  = epochs[:, name_col].astype(str)\n",
    "    is_sp  = np.char.find(names, 'sparse') >= 0\n",
    "\n",
    "    for row in epochs[is_sp]:\n",
    "        start = int(row[start_col])\n",
    "        end   = int(row[end_col])\n",
    "        mask[start:end] = True\n",
    "\n",
    "    return mask\n",
    "\n",
    "def compute_qc_metrics(dff):\n",
    "    \"\"\"\n",
    "    Compute per-cell ΔF/F variance and a simple SNR metric.\n",
    "    Returns (variance array, snr array) of length n_cells.\n",
    "    \"\"\"\n",
    "    var = np.var(dff, axis=1)\n",
    "    p98 = np.percentile(dff, 98, axis=1)\n",
    "    p02 = np.percentile(dff,  2, axis=1)\n",
    "    noise_std = np.std(dff, axis=1)\n",
    "    snr = (p98 - p02) / (noise_std + 1e-8)\n",
    "    return var, snr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02382b65",
   "metadata": {},
   "source": [
    "# 3.1 Drift removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f60aaff4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "isolate_sparse_epochs() got an unexpected keyword argument 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m mask = \u001b[43misolate_sparse_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocally_sparse_noise\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m dff_sparse = data[\u001b[33m\"\u001b[39m\u001b[33mdff\u001b[39m\u001b[33m\"\u001b[39m][:, mask]\n\u001b[32m      3\u001b[39m t_sparse = data[\u001b[33m\"\u001b[39m\u001b[33mt\u001b[39m\u001b[33m\"\u001b[39m][mask]\n",
      "\u001b[31mTypeError\u001b[39m: isolate_sparse_epochs() got an unexpected keyword argument 'target'"
     ]
    }
   ],
   "source": [
    "mask = isolate_sparse_epochs(data, target=\"locally_sparse_noise\")\n",
    "dff_sparse = data[\"dff\"][:, mask]\n",
    "t_sparse = data[\"t\"][mask]\n",
    "print(f\"Sparse‑noise frames: {mask.sum()}\")\n",
    "\n",
    "var_raw, snr_raw = compute_qc_metrics(dff_sparse)\n",
    "\n",
    "data.update(\n",
    "    {\n",
    "        \"mask_sparse\": mask,\n",
    "        \"dff_sparse\": dff_sparse,\n",
    "        \"t_sparse\": t_sparse,\n",
    "        \"var_raw\": var_raw,\n",
    "        \"snr_raw\": snr_raw,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6db690ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dff_sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Step Neuropil Estimation & Robust Regression\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Grab all sparse‐noise ΔF/F traces\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m dff_sparse_all = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdff_sparse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# shape: (n_cells, n_time)\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Estimate neuropil proxy (mean of other cells)\u001b[39;00m\n\u001b[32m      8\u001b[39m neuropil_proxy = estimate_neuropil_proxy(dff_sparse_all)\n",
      "\u001b[31mKeyError\u001b[39m: 'dff_sparse'"
     ]
    }
   ],
   "source": [
    "# Step Neuropil Estimation & Robust Regression\n",
    "\n",
    "\n",
    "# Grab all sparse‐noise ΔF/F traces\n",
    "dff_sparse_all = data[\"dff_sparse\"]  # shape: (n_cells, n_time)\n",
    "\n",
    "# Estimate neuropil proxy (mean of other cells)\n",
    "neuropil_proxy = estimate_neuropil_proxy(dff_sparse_all)\n",
    "\n",
    "# Perform robust per‐cell regression & clamp ρ to [0.5, 0.9]\n",
    "dff_regressed, rho_neuropil = regress_neuropil_robust(\n",
    "    dff_sparse_all, neuropil_proxy, clamp=(0.0, 0.9)\n",
    ")\n",
    "\n",
    "# Store results for downstream steps\n",
    "data.update(\n",
    "    {\n",
    "        \"neuropil_proxy\": neuropil_proxy,\n",
    "        \"dff_regressed\": dff_regressed,\n",
    "        \"rho_neuropil\": rho_neuropil,\n",
    "    }\n",
    ")\n",
    "\n",
    "# 5) Quick sanity check\n",
    "print(f\"Neuropil regression done for {dff_regressed.shape[0]} cells.\")\n",
    "print(\"Example ρ values (first 5 cells):\", rho_neuropil[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad2d6ef9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dff_regressed'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Drift Removal via Sliding‐Window Percentile Baseline\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# inputs: neuropil‐regressed ΔF/F and its timebase\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m dff_in = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdff_regressed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# shape: (n_cells, n_time)\u001b[39;00m\n\u001b[32m      5\u001b[39m tvec = data[\u001b[33m\"\u001b[39m\u001b[33mt_sparse\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# shape: (n_time,)\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# compute & subtract 10th‐percentile baseline over 60 s windows\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'dff_regressed'"
     ]
    }
   ],
   "source": [
    "# Drift Removal via Sliding‐Window Percentile Baseline\n",
    "\n",
    "# inputs: neuropil‐regressed ΔF/F and its timebase\n",
    "dff_in = data[\"dff_regressed\"]  # shape: (n_cells, n_time)\n",
    "tvec = data[\"t_sparse\"]  # shape: (n_time,)\n",
    "\n",
    "# compute & subtract 10th‐percentile baseline over 60 s windows\n",
    "baseline, dff_drift, win_frames = sliding_baseline(\n",
    "    dff_in,\n",
    "    tvec,\n",
    "    window_sec=60,  # window length in seconds\n",
    "    pct=10,  # percentile for baseline\n",
    "    mode=\"nearest\",  # boundary handling\n",
    ")\n",
    "\n",
    "# store for downstream steps\n",
    "data.update(\n",
    "    {\n",
    "        \"dff_baseline\": baseline,\n",
    "        \"dff_drift\": dff_drift,\n",
    "        \"drift_win\": win_frames,\n",
    "    }\n",
    ")\n",
    "\n",
    "# report\n",
    "dt = np.median(np.diff(tvec))\n",
    "print(f\"Drift‐window: {win_frames} frames (~{win_frames*dt:.1f} s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6de39c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Savitzky–Golay Smoothing\n",
    "from utils import smooth_dff_savgol\n",
    "\n",
    "sg_window = 11  # frames\n",
    "sg_poly = 2  # polynomial order\n",
    "\n",
    "# pull the drift‑corrected traces\n",
    "dff_dc = data[\"dff_drift\"]  # shape (n_cells, n_time)\n",
    "\n",
    "# smooth them\n",
    "dff_smooth = smooth_dff_savgol(dff_dc, window=sg_window, polyorder=sg_poly)\n",
    "\n",
    "# store back\n",
    "data[\"dff_smooth\"] = dff_smooth\n",
    "\n",
    "print(f\"Savitzky–Golay smoothing → window={sg_window}, polyorder={sg_poly}\")\n",
    "print(\"Smoothed shape:\", dff_smooth.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tue-summer-2025-oasis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
